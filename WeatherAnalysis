#Python API bindings PySpark is all set for use. To use SQL functions, need to instantiate SQL Context
from pyspark.sql import SQLContext
from pyspark.sql import functions
from pyspark.sql.types import *
sqlCtx = SQLContext(sc)

#Read list of all world-wide weather stations. See a sample of the stations

cefFile = sc.textFile("ghcnd-stations.txt")
stationsRDD = cefFile.map(lambda line: (line[0:11],line[41:63]))

#Lets concentrate on the maximum temperature for the day

dsWeatherData=sqlCtx.sql("Select * from WeatherData where Type='TMAX'").cache()

#What is the size of the dataset?
print(dsWeatherData.count())

#TMAX Comparison from Year 1970 and Year 2016
dsTempData=dsWeatherData.selectExpr("Station","Cast(Date as int) as dateint","Value/10 as TMAX","substring(Date,1,4) as DateYear").cache()

dsAvgData=dsTempData.groupby("DateYear").avg("TMAX")

dsAvgData.show() 

